{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_7",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbamman/nlp20/blob/master/HW_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE4-tSuL0G64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "import torch.nn as nn\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqTbFUId0MSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if this cell prints \"Running on cpu\", you must switch runtime environments\n",
        "# go to Runtime > Change runtime type > Hardware accelerator > GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on {}\".format(device))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-CW1Dvi9FYx",
        "colab_type": "text"
      },
      "source": [
        "# Data processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2fVAg4f0QKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This gets the EmpatheticDialogues corpus\n",
        "!wget https://dl.fbaipublicfiles.com/parlai/empatheticdialogues/empatheticdialogues.tar.gz\n",
        "!tar -xvf empatheticdialogues.tar.gz\n",
        "#This gets the GLoVe embeddings, which we will use to bootstrap our model\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8shZmBg6Yxl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PAD_INDEX = 0             # reserved for padding words\n",
        "UNKNOWN_INDEX = 1         # reserved for unknown words\n",
        "START_DECODE = 2          # special symbol to denote decoding should start\n",
        "END_DECODE = 3            # special symbol to indicate decoding is ending. This is how the model indicates the sequence is done.\n",
        "\n",
        "def read_embeddings(filename, vocab_size=10000):\n",
        "  \"\"\"\n",
        "  Utility function, loads in the `vocab_size` most common embeddings from `filename`\n",
        "  \n",
        "  Arguments:\n",
        "  - filename:     path to file\n",
        "                  automatically infers correct embedding dimension from filename\n",
        "  - vocab_size:   maximum number of embeddings to load\n",
        "\n",
        "  Returns \n",
        "  - embeddings:   torch.FloatTensor matrix of size (vocab_size x word_embedding_dim)\n",
        "  - vocab:        dictionary mapping word (str) to index (int) in embedding matrix\n",
        "  \"\"\"\n",
        "\n",
        "  # get the embedding size from the first embedding\n",
        "  with open(filename, encoding=\"utf-8\") as file:\n",
        "    word_embedding_dim = len(file.readline().split(\" \")) - 1\n",
        "\n",
        "  vocab = {}\n",
        "  vocab[\"PAD_INDEX\"] = 0\n",
        "  vocab[\"UNKNOWN_INDEX\"] = 1\n",
        "  vocab[\"START_DECODE\"] = 2\n",
        "  vocab[\"END_DECODE\"] = 3\n",
        "\n",
        "  embeddings = np.zeros((vocab_size, word_embedding_dim))\n",
        "\n",
        "  with open(filename, encoding=\"utf-8\") as file:\n",
        "    for idx, line in enumerate(file):\n",
        "      if idx + 4 >= vocab_size:\n",
        "        break\n",
        "\n",
        "      cols = line.rstrip().split(\" \")\n",
        "      val = np.array(cols[1:])\n",
        "      word = cols[0]\n",
        "      embeddings[idx + 4] = val\n",
        "      vocab[word] = idx + 4\n",
        "  \n",
        "  # a FloatTensor is a multidimensional matrix\n",
        "  # that contains 32-bit floats in every entry\n",
        "  # https://pytorch.org/docs/stable/tensors.html\n",
        "  return torch.FloatTensor(embeddings), vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbNf4hcpnXIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's load in a spacy tokenizer to process our conversation data\n",
        "tokenizer = get_tokenizer(\"spacy\")\n",
        "\n",
        "class Dataset():\n",
        "  '''\n",
        "  This is a Dataset object, similar to the one used in HW4.\n",
        "\n",
        "  It serves two purposes- reading data and creating batches.\n",
        "\n",
        "  read_data():\n",
        "    Inputs: \n",
        "      filename \n",
        "      emotions_list- list of emotions to include in this dataset\n",
        "    Outputs: \n",
        "      emotions-     list of emotions associated with the dataset\n",
        "      past_turns-   list of past turns associated with the dataset.  \n",
        "                    This is input to our model.\n",
        "      responses-    list of responses associated with the dataset.  This is what \n",
        "                    we will train our model to generate.\n",
        "\n",
        "  get_batches():\n",
        "    Inputs: batch_size- size of batches we want to create \n",
        "            vocab-      our vocabulary, used to replace unknown words\n",
        "            emotset-    set of emotions to id, used to create emotion IDs\n",
        "    Outputs:  \n",
        "      batched_past_turn_idx:      indices of the words in the past turn\n",
        "      batched_past_lengths:       lengths of the past turns (since we are padding these)\n",
        "      batched_response_idx:       indices of words in the response\n",
        "      batched_past_resp_lengths:  lengths of the response (since we pad those)\n",
        "      batched_emotions:           emotions associated with the conversations\n",
        "\n",
        "  '''\n",
        "  def __init__(self, filename, emotions_list=None):\n",
        "    if emotions_list is not None:\n",
        "      self.emotions_list = ['context'] + emotions_list\n",
        "    else:\n",
        "      self.emotions_list = None\n",
        "    self.emotions, self.past_turns, self.responses = self.read_data(filename)\n",
        "\n",
        "  def read_data(self, filename):\n",
        "    past_turns = []\n",
        "    responses = []\n",
        "    emotions = []\n",
        "\n",
        "    raw_data = {}\n",
        "\n",
        "    with open(filename, encoding='utf8') as f:\n",
        "      csvreader = csv.reader(f, delimiter=',')\n",
        "      for row in csvreader:\n",
        "        convo_num = row[0]\n",
        "        emotion = row[2]  \n",
        "        utterance = row[5]\n",
        "        #if we receive an emotions_list, we need to make sure the emotion is relevant to the Dataset we create.\n",
        "        if self.emotions_list is None or emotion in self.emotions_list:\n",
        "          if convo_num not in raw_data:\n",
        "            raw_data[convo_num] = {}\n",
        "            raw_data[convo_num][\"emotion\"] = emotion\n",
        "            raw_data[convo_num][\"convo\"] = []\n",
        "          raw_data[convo_num][\"convo\"].append(utterance.replace(\"_comma_\", \",\"))\n",
        "\n",
        "      for key in raw_data:\n",
        "        for i,turn in enumerate(raw_data[key][\"convo\"]):\n",
        "          #we want to grab every other response\n",
        "          if i % 2 == 0 and i != 0:\n",
        "            emotions.append(raw_data[key][\"emotion\"])\n",
        "            past_turns.append(tokenizer(raw_data[key][\"convo\"][i-1].lower()))\n",
        "            responses.append(tokenizer(raw_data[key][\"convo\"][i].lower()))\n",
        "    \n",
        "    return emotions, past_turns, responses\n",
        "\n",
        "  def get_batches(self, batch_size, vocab, emotset):\n",
        "    # randomly shuffle the data\n",
        "    np.random.seed(159) # don't change this, for reproducibility\n",
        "    shuffle = np.random.permutation(range(len(self.past_turns)))\n",
        "    \n",
        "    #grabs the relevant data from the random permutation\n",
        "    past_turns = [self.past_turns[i] for i in shuffle]\n",
        "    emotions = [self.emotions[i] for i in shuffle]\n",
        "    responses = [self.responses[i] for i in shuffle]\n",
        "\n",
        "    #stores the id's of past_turn words\n",
        "    batched_past_turn_idx = []\n",
        "    #stores the id's of response words\n",
        "    batched_response_idx = []\n",
        "    #stores the lengths of past_turns for masking\n",
        "    batched_past_lengths = []\n",
        "    #stores the lengths of responses for masking\n",
        "    batched_past_resp_lengths = []\n",
        "    #stores the emotions associated with a batch\n",
        "    batched_emotions = []\n",
        "\n",
        "    #creates batches\n",
        "    N = len(past_turns)\n",
        "    if N % batch_size == 0:\n",
        "      num_batches = N // batch_size\n",
        "    else:\n",
        "      num_batches = N // batch_size + 1\n",
        "\n",
        "    for b in range(num_batches):\n",
        "      start = b * batch_size\n",
        "      stop = min((b+1) * batch_size, len(past_turns))\n",
        "      #calculates the max lengths of response and past turn sequences for this batch\n",
        "      max_resp_seq_len = max([len(s) for s in responses[start:stop]])\n",
        "      max_past_seq_len = max([len(s) for s in past_turns[start:stop]])\n",
        "\n",
        "      #creates the vectors for the past_turn and responses\n",
        "      past_turn_idx = np.zeros((stop-start, max_past_seq_len))\n",
        "      response_idx = np.zeros((stop-start, max_resp_seq_len + 2))\n",
        "      emotion_idx = np.empty((stop-start, 1))\n",
        "      past_lengths = np.zeros((stop-start))\n",
        "      resp_lengths = np.zeros((stop-start))\n",
        "      for i in range(start, stop):\n",
        "        #gathers the corresponding data\n",
        "        past_turn = past_turns[i]\n",
        "        response = responses[i]\n",
        "        emotion = emotions[i]\n",
        "        #gets ID for corresponding emotion\n",
        "        emotion_idx[i - start] = emotset[emotion]\n",
        "\n",
        "        #We start the response with START_DECODE to indicate to the model that decoding should start\n",
        "        response_idx[i - start][0] = START_DECODE\n",
        "\n",
        "        #this captures the lengths \n",
        "        past_lengths[i - start] = len(past_turn)\n",
        "        resp_lengths[i - start] = len(response)\n",
        "\n",
        "        #this gets the vocabulary IDs for each word in the past_turn and response\n",
        "        #UNKNOWN_INDEX is used if the word is out of vocabulary\n",
        "        for j in range(len(past_turn)):\n",
        "          if past_turn[j] in vocab:\n",
        "            past_turn_idx[i - start][j] = vocab[past_turn[j]]\n",
        "          else:\n",
        "            past_turn_idx[i - start][j] = UNKNOWN_INDEX      \n",
        "        for j in range(len(response)):\n",
        "          if response[j] in vocab:\n",
        "            response_idx[i - start][j + 1] = vocab[response[j]]\n",
        "          else:\n",
        "            response_idx[i - start][j + 1] = UNKNOWN_INDEX\n",
        "            \n",
        "        #we want to end the response with END_DECODE so the model learns to predict the end of an utterance\n",
        "        response_idx[i - start][len(response)] = END_DECODE\n",
        "      batched_past_turn_idx.append(past_turn_idx)\n",
        "      batched_response_idx.append(response_idx)\n",
        "      batched_past_lengths.append(past_lengths)\n",
        "      batched_past_resp_lengths.append(resp_lengths)\n",
        "      batched_emotions.append(emotion_idx)\n",
        "    return batched_past_turn_idx, batched_past_lengths, batched_response_idx, batched_past_resp_lengths, batched_emotions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhjaUWnh4xZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Helper function to extract a set of emotions from a dataset and associate them \n",
        "with an ID.\n",
        "\n",
        "Arguments-\n",
        "  emotion_file:   Data file we want to extract emotions from\n",
        "\n",
        "Returns:\n",
        "  emotset:        Dictionary of emotions\n",
        "'''\n",
        "def read_emotions(emotion_file):\n",
        "  emotset = {}\n",
        "  with open(emotion_file, encoding='utf8') as f:\n",
        "    csvreader = csv.reader(f, delimiter=',')\n",
        "    counter = 0\n",
        "    for row in csvreader:\n",
        "      emotion = row[2]\n",
        "      if emotion not in emotset:\n",
        "        emotset[emotion] = counter\n",
        "        counter += 1\n",
        "  \n",
        "  return emotset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5SeNJvK5Q_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this loads the 10,000 most common word 300-dimensional embeddings\n",
        "vocab_size = 10000\n",
        "embeddings, vocab = read_embeddings('glove.6B.300d.txt', vocab_size)\n",
        "\n",
        "# read the files\n",
        "emotset = read_emotions('empatheticdialogues/train.csv')\n",
        "train_dataset = Dataset('empatheticdialogues/train.csv')\n",
        "dev_dataset = Dataset('empatheticdialogues/valid.csv')\n",
        "test_dataset = Dataset('empatheticdialogues/test.csv')\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_batched_past_turn_idx, train_batched_past_lengths, train_batched_response_idx, train_batched_past_resp_lengths, train_batched_emotions = train_dataset.get_batches(BATCH_SIZE, vocab, emotset)\n",
        "dev_batched_past_turn_idx, dev_batched_past_lengths, dev_batched_response_idx, dev_batched_past_resp_lengths, dev_batched_emotions= dev_dataset.get_batches(BATCH_SIZE, vocab, emotset)\n",
        "test_batched_past_turn_idx, test_batched_past_lengths, test_batched_response_idx, test_batched_past_resp_lengths, test_batched_emotions= test_dataset.get_batches(BATCH_SIZE, vocab, emotset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQgF4Ni89KGL",
        "colab_type": "text"
      },
      "source": [
        "# Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik4JzhLS83N-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "This class is used to create transformer-style positional encodings.  \n",
        "Reference: https://github.com/pytorch/pytorch/issues/24826\n",
        "Note: these are different than the categorical positional encodings discussed in \n",
        "class for Information Extraction.\n",
        "'''\n",
        "class TransformerPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=160):\n",
        "        super(TransformerPositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        #import pdb; pdb.set_trace()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "'''\n",
        "This is the TransformerGenerator class, where the generaiton model is set up \n",
        "and the model structure is defined.\n",
        "\n",
        "Please fill in your solution where you see \"...\" \n",
        "'''\n",
        "class TransformerGenerator(nn.Module):\n",
        "  def __init__(self, embeddings, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embed = nn.Embedding.from_pretrained(embeddings, freeze=False)\n",
        "\n",
        "    self.model = ...\n",
        "    self.out = ...\n",
        "    self.pos_encoder = ...\n",
        "    self.pos_decoder = ...\n",
        "\n",
        "  def forward(self, past_turn, past_turn_lengths, response, response_lengths):\n",
        "\n",
        "    past_turn = torch.LongTensor(past_turn).to(device)\n",
        "    response = torch.LongTensor(response).to(device)\n",
        "    past_turn_lengths = torch.LongTensor(past_turn_lengths)\n",
        "    response_lengths = torch.LongTensor(response_lengths)\n",
        "\n",
        "    src_masks = ...\n",
        "    tgt_masks = ...\n",
        "    src = ...\n",
        "    tgt = ...\n",
        "\n",
        "    #Ensures decoder doesn't peek at the future tokens\n",
        "    cheater_mask = self.model.generate_square_subsequent_mask(sz = len(trg)).to(device)\n",
        "\n",
        "    output = self.model(src, trg, tgt_mask = cheater_mask, src_key_padding_mask=src_masks.to(device), tgt_key_padding_mask = tgt_masks.to(device))\n",
        "    output = self.out(output)\n",
        "\n",
        "    return output\n",
        "\n",
        "'''\n",
        "This is a function which is used to evaluate a model on a development dataset.\n",
        "This method does not update the model; rather, it is used to evaluate a model's responses\n",
        "on a dataset.\n",
        "\n",
        "Arguments:\n",
        "  model:        Model to evaluate\n",
        "  dev_dataset:  Dataset we want to evaluate with\n",
        "  batch_size:   batch size for dev dataset\n",
        "  vocab:        Vocabulary for the dataset\n",
        "  emotset:      Set of emotions for the dataset\n",
        "\n",
        "Returns:\n",
        "  avg_loss:     The average loss for the model on the dev_dataset.\n",
        "'''\n",
        "def evaluate_on_data(model, dev_dataset, batch_size, vocab,  emotset):\n",
        "    loss_function = nn.CrossEntropyLoss(ignore_index=vocab[\"PAD_INDEX\"])\n",
        "    dev_batched_past_turn_idx, dev_batched_past_lengths, dev_batched_response_idx, dev_batched_resp_lengths, dev_batched_emotions= dev_dataset.get_batches(BATCH_SIZE, vocab, emotset)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "      total_loss = 0\n",
        "      for b in range(len(dev_batched_past_turn_idx)):\n",
        "          # have to transpose since model expects them in a certain format\n",
        "          src = dev_batched_past_turn_idx[b].transpose([1, 0])\n",
        "          tgt = dev_batched_response_idx[b].transpose([1, 0])\n",
        "\n",
        "          #calls the model on the current batch's input\n",
        "          logits = model.forward(src, dev_batched_past_lengths[b], tgt[:-1,:], dev_batched_resp_lengths[b])\n",
        "          # move labels to GPU memory\n",
        "          labels = torch.LongTensor(dev_batched_response_idx[b].transpose([1,0])).to(device)\n",
        "          # compute the loss with respect to true words\n",
        "          loss = loss_function(logits.view(-1, 10000), labels[1:,:].view(-1))\n",
        "          total_loss += loss\n",
        "      avg_loss = total_loss / float(len(dev_batched_past_turn_idx))\n",
        "      avg_loss = float(avg_loss.cpu().numpy())\n",
        "      return avg_loss\n",
        "\n",
        "'''\n",
        "This is the function used to train a model.\n",
        "\n",
        "Arguments:\n",
        "  model:          model we want to train\n",
        "  train_dataset:  dataset we want to train the model with\n",
        "  dev_dataset:    dataset we want to evaluate model with during training\n",
        "  batch_size:     batch size for training\n",
        "  vocab:          vocabulary for the dataset\n",
        "  emotset:        emotion set for the dataset\n",
        "  lr:             learning rate we want to use\n",
        "  num_epochs:     epochs we want to train our model for\n",
        "  eval_every:     how often we want to evaluate on the dev dataset\n",
        "'''\n",
        "def run_training(model, train_dataset, dev_dataset, batch_size, vocab,  emotset,\n",
        "                         lr=1e-4, num_epochs=100, eval_every=5):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    if str(device) == 'cpu':\n",
        "      print(\"Training only supported in GPU environment\")\n",
        "      return\n",
        "\n",
        "\n",
        "    # clear unreferenced data/models from GPU memory \n",
        "    torch.cuda.empty_cache()\n",
        "    # move model to GPU memory\n",
        "    model.to(device)\n",
        "\n",
        "    # set the optimizer (Adam) and loss function (CrossEnt)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_function = nn.CrossEntropyLoss(ignore_index=vocab[\"PAD_INDEX\"])\n",
        "\n",
        "    # batch training and dev data\n",
        "    train_batched_past_turn_idx, train_batched_past_lengths, train_batched_response_idx, train_batched_resp_lengths, train_batched_emotions = train_dataset.get_batches(BATCH_SIZE, vocab, emotset)\n",
        "    #dev_batched_past_turn_idx, dev_batched_past_lengths, dev_batched_response_idx, dev_batched_resp_lengths, dev_batched_emotions= dev_dataset.get_batches(BATCH_SIZE, vocab, emotset)\n",
        "\n",
        "    t0 = time.time()\n",
        "    print(\"**** TRAINING *****\")\n",
        "    for i in range(num_epochs):\n",
        "      if i % eval_every == 0:\n",
        "      #  # Run on Dev data\n",
        "         dev_loss = evaluate_on_data(model, dev_dataset, batch_size, vocab,  emotset)\n",
        "         print(\"-------------------------------\")\n",
        "         print(\"Dev Loss: {}\".format(dev_loss))\n",
        "         print(\"-------------------------------\")\n",
        "\n",
        "      # sets the model in train mode\n",
        "      model.train()\n",
        "      total_loss = 0\n",
        "      for b in range(len(train_batched_past_turn_idx)):\n",
        "\n",
        "        # have to transpose since model expects them in a certain format\n",
        "        src = train_batched_past_turn_idx[b].transpose([1, 0])\n",
        "        tgt = train_batched_response_idx[b].transpose([1, 0])\n",
        "\n",
        "        #calls the model on the current batch's input\n",
        "        logits = model.forward(src, train_batched_past_lengths[b], tgt[:-1,:], train_batched_resp_lengths[b])\n",
        "        # move labels to GPU memory\n",
        "        labels = torch.LongTensor(train_batched_response_idx[b].transpose([1,0])).to(device)\n",
        "        # compute the loss with respect to true words\n",
        "        loss = loss_function(logits.view(-1, 10000), labels[1:,:].view(-1))\n",
        "        total_loss += loss\n",
        "\n",
        "        # propagate gradients backward\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # set model gradients to zero before performing next forward pass\n",
        "        model.zero_grad()\n",
        "\n",
        "      seconds_elapsed = time.time()-t0\n",
        "      mins = int(np.floor(seconds_elapsed/60))\n",
        "      secs = int(seconds_elapsed - (60*mins))\n",
        "      print(\"Epoch {} | Train Loss: {} | Time: {} mins, {} secs\".format(i, total_loss / float(len(train_batched_past_turn_idx)),mins,secs))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e1LiLFn-mZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_seed(seed):\n",
        "  \"\"\"\n",
        "  Sets random seeds and sets model in deterministic\n",
        "  training mode. Ensures reproducible results\n",
        "  \"\"\"\n",
        "  torch.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  np.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRELt-cKm5RI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sets the random seed – DO NOT change this\n",
        "# this ensures deterministic results that are comparable with the staff values\n",
        "set_seed(159)\n",
        "\n",
        "'''Do NOT change these parameters'''\n",
        "#Number of vocabulary words we have\n",
        "VOCAB_SIZE = 10000\n",
        "#Size of our word embeddings.  We embed each word before passing into the transformer layer,\n",
        "#so the transformer needs to know how large these embeddings will be\n",
        "NINP = 300\n",
        "#The number of heads we want our transformer model to have\n",
        "NHEAD = 6\n",
        "#The size of hidden dimensions we want our transformer to have\n",
        "NHIDDEN = 200\n",
        "#The number of layers we want our transformer to have\n",
        "NLAYERS = 1\n",
        "#Dropout rate\n",
        "DROPOUT = 0.4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsv_Rtks35gS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "NOTE: do NOT run this cell if you are loading a pre-trained model.\n",
        "'''\n",
        "\n",
        "#This is the call which initializes the model\n",
        "model = TransformerGenerator(embeddings, VOCAB_SIZE, NINP, NHEAD, NHIDDEN, NLAYERS, DROPOUT)\n",
        "\n",
        "# This call trains the model.  If you have implemented Q1 correctly, the loss should decrease from ~5.6 to ~3.8\n",
        "# Sanity check: if this function fails, your Q1 code is probably incorrect. \n",
        "run_training(model, train_dataset, dev_dataset, BATCH_SIZE, vocab, emotset, \n",
        "                   lr=1e-4, num_epochs=25, eval_every=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRm_VBOrOHPe",
        "colab_type": "text"
      },
      "source": [
        "**If you want to save your trained model so you don't have to train it again for #2, please run the following cell.  You will need to download the model file and import this to Colab the next time you'd like to load it.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss4X5RHFxQIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now, let's save this model so you won't have to run it again for #2.\n",
        "torch.save(model.state_dict(), \"./model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh4jVENRxfRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#NOTE: if you have a saved version of the model, un-comment and run this code to load your model back in.\n",
        "# model = TransformerGenerator(embeddings, VOCAB_SIZE, NINP, NHEAD, NHIDDEN, NLAYERS, DROPOUT)\n",
        "# model.load_state_dict(torch.load(\"./model\"))\n",
        "# model.eval()\n",
        "# model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfRZScz17wLC",
        "colab_type": "text"
      },
      "source": [
        "# Decoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZcBsYZgAl89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def id2string(vocab, response):\n",
        "\n",
        "    \"\"\"\n",
        "    id2string function, takes a vocabulary and response and translates the response to a list of strings using the vocab.\n",
        "    \n",
        "    Arguments:\n",
        "    - vocab:        vocabulary, keys are strings and values are IDs\n",
        "    - response:     list of IDs we want to translate\n",
        "\n",
        "    Returns \n",
        "    - str_response:   list of strings containing the words represented by the response's IDs\n",
        "    \"\"\"\n",
        "    str_response = []\n",
        "    \n",
        "    return str_response\n",
        "\n",
        "    \n",
        "    \n",
        "def decode(model, prev_turn, prev_lengths, vocab, max_len, random_top_k=False):#, batched_resps, batched_resp_lens, vocab, max_len):\n",
        "    \"\"\"\n",
        "    Decode function, takes a trained model and past_turn and returns the model's generated response\n",
        "    \n",
        "    Arguments:\n",
        "    - model:        trained model that we want to evaluate\n",
        "    - prev_turn:    The previous turn we want to generate a model response for\n",
        "    - prev_lengths: The length of the prev_turn\n",
        "    - max_len:      The maximum decoded sequence length\n",
        "    - random_top_k: Flag specifying whether to use topK decoding\n",
        "\n",
        "    Returns \n",
        "    - id2string(vocab, prediction):   list of strings indicating the words produced \n",
        "                                      by the model, calculated using id2string helper function. \n",
        "    \"\"\"\n",
        "\n",
        "    prediction = []\n",
        "            \n",
        "    return id2string(vocab, prediction)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZRfpe9nzrgW",
        "colab_type": "text"
      },
      "source": [
        "### Let's test the base decode() function by evaluating it with some model responses to dev set data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emS1i7NB806j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "This function runs a trained model to respond to random examples from a dataset\n",
        "\n",
        "Arguments:\n",
        "  dataset:      Dataset we want to evaluate the model with\n",
        "  model:        The model we want to evaluate\n",
        "  random_top_k: Whether we want to use topk decoding\n",
        "'''\n",
        "def generate_5_responses(dataset, model, random_top_k=False):\n",
        "  dev_batched_past_turn_idx, dev_batched_past_lengths, dev_batched_response_idx, dev_batched_resp_lengths, dev_batched_emotions= dataset.get_batches(BATCH_SIZE, vocab, emotset)\n",
        "  for _ in range(5):\n",
        "    rand_batch = random.randint(0, len(dev_batched_past_turn_idx)-1)\n",
        "    rand_item = random.randint(0, len(dev_batched_past_turn_idx[0])-1)\n",
        "    print(\"Past response: \")\n",
        "    print(id2string(vocab, [x for x in dev_batched_past_turn_idx[rand_batch][rand_item] if x != PAD_INDEX]))\n",
        "    print(\"Model Response: \")\n",
        "    model_resp = decode(model=model, prev_turn=[dev_batched_past_turn_idx[rand_batch][rand_item]], prev_lengths=[dev_batched_past_lengths[rand_batch][rand_item]], vocab=vocab, max_len=20, random_top_k=random_top_k)\n",
        "    print(model_resp)\n",
        "    print(\"Gold Response: \")\n",
        "    print(id2string(vocab, [x for x in dev_batched_response_idx[rand_batch][rand_item][1:] if x != PAD_INDEX]))\n",
        "    print()\n",
        "    print(\"---------------------------------\")\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5-as-xwYkXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generate_5_responses(dev_dataset, model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2R6gNzvz9zS",
        "colab_type": "text"
      },
      "source": [
        "### Now, let's compare the base decode() method to the Random-Top5 decoding method.  The two model responses should be different."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmqcOyzR0ITO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generate_5_responses(dev_dataset, model, random_top_k=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCkPZwaP7yLT",
        "colab_type": "text"
      },
      "source": [
        "# Effect of Emotions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dWUtkoJDgEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Here is where we divide up the emotions in the emotset into positive and negative emotions, of equal sizes.\n",
        "positive_emotions = ['anticipating', 'caring', 'confident', 'content', 'excited', 'faithful', 'grateful', 'hopeful', 'impressed', 'joyful', 'nostalgic', 'prepared', 'proud', 'sentimental','surprised','trusting']\n",
        "negative_emotions = ['afraid', 'angry', 'annoyed', 'anxious', 'apprehensive', 'ashamed','devastated','disappointed','disgusted', 'embarrassed','furious','guilty','jealous','lonely','sad','terrified']\n",
        "len(positive_emotions), len(negative_emotions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKMtZHLfDgSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creates the positive and negative datasets, specifying the relevant emotions_list\n",
        "train_dataset_positive = Dataset('empatheticdialogues/train.csv', emotions_list=positive_emotions)\n",
        "dev_dataset_positive = Dataset('empatheticdialogues/valid.csv', emotions_list=positive_emotions)\n",
        "test_dataset_positive = Dataset('empatheticdialogues/test.csv', emotions_list=positive_emotions)\n",
        "\n",
        "train_dataset_negative = Dataset('empatheticdialogues/train.csv', emotions_list=negative_emotions)\n",
        "dev_dataset_negative = Dataset('empatheticdialogues/valid.csv', emotions_list=negative_emotions)\n",
        "test_dataset_negative = Dataset('empatheticdialogues/test.csv', emotions_list=negative_emotions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLyS6YVuLO4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "NOTE: these models train quicker than the Q1 model, as they are trained on 1/2 of the data.\n",
        "'''\n",
        "\n",
        "#we need to reset these embeddings or they will be shared among all 3 models.\n",
        "positive_embeddings, _ = read_embeddings('glove.6B.300d.txt', vocab_size)\n",
        "negative_embeddings, _ = read_embeddings('glove.6B.300d.txt', vocab_size)\n",
        "\n",
        "#This is the call which initializes the model\n",
        "positive_model = TransformerGenerator(positive_embeddings, VOCAB_SIZE, NINP, NHEAD, NHIDDEN, NLAYERS, DROPOUT)\n",
        "negative_model = TransformerGenerator(negative_embeddings, VOCAB_SIZE, NINP, NHEAD, NHIDDEN, NLAYERS, DROPOUT)\n",
        "\n",
        "print(\"Positive model\")\n",
        "run_training(positive_model, train_dataset_positive, dev_dataset_positive, BATCH_SIZE, vocab, emotset, \n",
        "                   lr=1e-4, num_epochs=25, eval_every=5)\n",
        "print(\"Negative model\")\n",
        "run_training(negative_model, train_dataset_negative, dev_dataset_negative, BATCH_SIZE, vocab, emotset, \n",
        "                   lr=1e-4, num_epochs=25, eval_every=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmNYBkRWynJL",
        "colab_type": "text"
      },
      "source": [
        "### Let's decode the *positive* model on some *positive* data to see the types of responses it produces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxhvkxSu8u_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generate_5_responses(dataset=dev_dataset_positive, model=positive_model, random_top_k=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCGDPLI5y02x",
        "colab_type": "text"
      },
      "source": [
        "### Let's decode the *negative* model on some *negative* data to see the types of responses it produces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpRmXr7NyvQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generate_5_responses(dataset=dev_dataset_negative, model=negative_model, random_top_k=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZCCUs8hy30V",
        "colab_type": "text"
      },
      "source": [
        "### Let's decode *both* models on some *positive* data to see the types of responses it produces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Znget6A-yvZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_batched_past_turn_idx, dev_batched_past_lengths, dev_batched_response_idx, dev_batched_resp_lengths, dev_batched_emotions= dev_dataset_positive.get_batches(BATCH_SIZE, vocab, emotset)\n",
        "for _ in range(5):\n",
        "  rand_batch = random.randint(0, len(dev_batched_past_turn_idx)-1)\n",
        "  rand_item = random.randint(0, len(dev_batched_past_turn_idx[0])-1)\n",
        "  print(\"Past response: \")\n",
        "  print(id2string(vocab, [x for x in dev_batched_past_turn_idx[rand_batch][rand_item] if x != PAD_INDEX]))\n",
        "  print(\"POSITIVE model Response: \")\n",
        "  model_resp = decode(model=positive_model, prev_turn=[dev_batched_past_turn_idx[rand_batch][rand_item]], prev_lengths=[dev_batched_past_lengths[rand_batch][rand_item]], vocab=vocab, max_len=20, random_top_k=True)\n",
        "  print(model_resp)\n",
        "  print(\"NEGATIVE model Response: \")\n",
        "  model_resp = decode(model=negative_model, prev_turn=[dev_batched_past_turn_idx[rand_batch][rand_item]], prev_lengths=[dev_batched_past_lengths[rand_batch][rand_item]], vocab=vocab, max_len=20, random_top_k=True)\n",
        "  print(model_resp)\n",
        "  print(\"Gold Response: \")\n",
        "  print(id2string(vocab, [x for x in dev_batched_response_idx[rand_batch][rand_item][1:] if x != PAD_INDEX]))\n",
        "  print()\n",
        "  print(\"---------------------------------\")\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjUfvnSwzEPb",
        "colab_type": "text"
      },
      "source": [
        "### Let's decode *both* models on some *negative* data to see the types of responses it produces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ccSl8SkyveX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_batched_past_turn_idx, dev_batched_past_lengths, dev_batched_response_idx, dev_batched_resp_lengths, dev_batched_emotions= dev_dataset_negative.get_batches(BATCH_SIZE, vocab, emotset)\n",
        "for _ in range(5):\n",
        "  rand_batch = random.randint(0, len(dev_batched_past_turn_idx)-1)\n",
        "  rand_item = random.randint(0, len(dev_batched_past_turn_idx[0])-1)\n",
        "  print(\"Past response: \")\n",
        "  print(id2string(vocab, [x for x in dev_batched_past_turn_idx[rand_batch][rand_item] if x != PAD_INDEX]))\n",
        "  print(\"POSITIVE model Response: \")\n",
        "  model_resp = decode(model=positive_model, prev_turn=[dev_batched_past_turn_idx[rand_batch][rand_item]], prev_lengths=[dev_batched_past_lengths[rand_batch][rand_item]], vocab=vocab, max_len=20, random_top_k=True)\n",
        "  print(model_resp)\n",
        "  print(\"NEGATIVE model Response: \")\n",
        "  model_resp = decode(model=negative_model, prev_turn=[dev_batched_past_turn_idx[rand_batch][rand_item]], prev_lengths=[dev_batched_past_lengths[rand_batch][rand_item]], vocab=vocab, max_len=20, random_top_k=True)\n",
        "  print(model_resp)\n",
        "  print(\"Gold Response: \")\n",
        "  print(id2string(vocab, [x for x in dev_batched_response_idx[rand_batch][rand_item][1:] if x != PAD_INDEX]))\n",
        "  print()\n",
        "  print(\"---------------------------------\")\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2Gy5akr0kTq",
        "colab_type": "text"
      },
      "source": [
        "### Now, let's see which model does better when evaluated on the other's development set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSnaRk490oq9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compare_positive_and_negative_data(positive_model, negative_model, dev_dataset_positive, dev_dataset_negative):\n",
        "  '''\n",
        "  This method compares the positive model on the negative dataset and the negative model on the positive dataset.\n",
        "  You should use evaluate_on_data to get the training loss for each model.\n",
        "\n",
        "  Arguments:\n",
        "  positive_model:       model trained on positive data\n",
        "  negative_model:       model trained on negative data\n",
        "  dev_dataset_positive: dev dataset for positive data\n",
        "  dev_dataset_negative: dev dataset for negative data\n",
        "\n",
        "  Returns:\n",
        "  positive_model_negative_data:   result of evaluating the positive model on the negative dev dataset\n",
        "  negative_model_positive_data:   result of evaluating the negative model on the positive dev dataset\n",
        "  '''\n",
        "  #Your code goes here. \n",
        "  \n",
        "  return positive_model_negative_data, negative_model_positive_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBORbA8oD2Xc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_model_negative_data, negative_model_positive_data = compare_positive_and_negative_data(positive_model,  \n",
        "                                                                                                negative_model, \n",
        "                                                                                                dev_dataset_positive, \n",
        "                                                                                                dev_dataset_negative)\n",
        "print(\"Positive model loss on negative data:\", positive_model_negative_data)\n",
        "print(\"Negative model loss on positive data:\", negative_model_positive_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AazxFndM_-j2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}