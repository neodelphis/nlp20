{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOKxnsd8HWWQQwKifXdcbPL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbamman/nlp20/blob/master/HW_4/HW_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5KTrLJKD-l7",
        "colab_type": "text"
      },
      "source": [
        "# Homework 4: Neural Sequence Labeling\n",
        "\n",
        "**Due March 4, 2020 at 11:59PM**\n",
        "\n",
        "\n",
        "In this homework, you will be implementing, training, and evaluating an LSTM for part-of-speech tagging using the PyTorch library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFgEH9BWEqPV",
        "colab_type": "text"
      },
      "source": [
        "**Before beginning, please switch your Colab session to a GPU runtime** \n",
        "\n",
        "Go to Runtime > Change runtime type > Hardware accelerator > GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xALTxzWIEkkE",
        "colab_type": "text"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk8GRWIkbrU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t87Vex5_b5MI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if this cell prints \"Running on cpu\", you must switch runtime environments\n",
        "# go to Runtime > Change runtime type > Hardware accelerator > GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on {}\".format(device))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3nDS9MwFCVf",
        "colab_type": "text"
      },
      "source": [
        "### Download & Load Pretrained Embeddings\n",
        "\n",
        "In this assignment, we will be using GloVe pretrained word embeddings. You can read more about GloVe here: https://nlp.stanford.edu/projects/glove/\n",
        "\n",
        "**Note**: this section will take *several minutes*, since the embedding files are large. Files in Colab may be cached between sessions, so you may or may not need to redownload the files each time you reconnect. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csWld6ckFNL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download pretrained word embeddings\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiuJ5eylL0A0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_embeddings(filename, vocab_size=10000):\n",
        "  \"\"\"\n",
        "  Utility function, loads in the `vocab_size` most common embeddings from `filename`\n",
        "  \n",
        "  Arguments:\n",
        "  - filename:     path to file\n",
        "                  automatically infers correct embedding dimension from filename\n",
        "  - vocab_size:   maximum number of embeddings to load\n",
        "\n",
        "  Returns \n",
        "  - embeddings:   torch.FloatTensor matrix of size (vocab_size x word_embedding_dim)\n",
        "  - vocab:        dictionary mapping word (str) to index (int) in embedding matrix\n",
        "  \"\"\"\n",
        "\n",
        "  # get the embedding size from the first embedding\n",
        "  with open(filename, encoding=\"utf-8\") as file:\n",
        "    word_embedding_dim = len(file.readline().split(\" \")) - 1\n",
        "\n",
        "  vocab = {}\n",
        "\n",
        "  embeddings = np.zeros((vocab_size, word_embedding_dim))\n",
        "\n",
        "  with open(filename, encoding=\"utf-8\") as file:\n",
        "    for idx, line in enumerate(file):\n",
        "\n",
        "      if idx + 2 >= vocab_size:\n",
        "        break\n",
        "\n",
        "      cols = line.rstrip().split(\" \")\n",
        "      val = np.array(cols[1:])\n",
        "      word = cols[0]\n",
        "      embeddings[idx + 2] = val\n",
        "      vocab[word] = idx + 2\n",
        "  \n",
        "  # a FloatTensor is a multidimensional matrix\n",
        "  # that contains 32-bit floats in every entry\n",
        "  # https://pytorch.org/docs/stable/tensors.html\n",
        "  return torch.FloatTensor(embeddings), vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ah3clY7lHNZ",
        "colab_type": "text"
      },
      "source": [
        "Running the cell below lists all the files in the current directory. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D__oK6mQ6hs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -lh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoSWrCwZllg6",
        "colab_type": "text"
      },
      "source": [
        "You should see several embedding files, which are all formatted as\n",
        "\n",
        "```\n",
        "glove.6B.<emb_dim>d.txt\n",
        "```\n",
        "\n",
        "Each `txt` file contains `emb_dim` dimensional embeddings for 400,000 unique, uncased words. The script below loads the `vocab_size` most common words from the embedding file into a matrix we can give to our model. All other words will later be mapped to the `UNKNOWN` embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL8WuEZoOFbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this loads the 10,000 most common word 50-dimensional embeddings\n",
        "vocab_size = 10000\n",
        "embeddings, vocab = read_embeddings('glove.6B.50d.txt', vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py9AStUOJnPB",
        "colab_type": "text"
      },
      "source": [
        "## Part 1: Batching the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5jWl-z8w_5t",
        "colab_type": "text"
      },
      "source": [
        "Implement the `get_batches` function in the `Dataset` class below. \n",
        "\n",
        "**Please make sure that**\n",
        "\n",
        "*   Your implementation is self-contained. That is, all helper functions and variables are defined within `get_batches`.\n",
        "*   Your implementation can handle variable batch sizes. You may not assume that the value with always be 32\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KWyqb2HcLop",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset():\n",
        "  def __init__(self, filename, is_labeled):\n",
        "    self.is_labeled = is_labeled\n",
        "    # if the file is not labeled, the Dataset has no tags (see read_data)\n",
        "    if is_labeled:\n",
        "      self.sentences, self.tags = self.read_data(filename, is_labeled)\n",
        "    else:\n",
        "      self.sentences = self.read_data(filename, is_labeled)\n",
        "      self.tags = None\n",
        "\n",
        "  def read_data(self, filename, is_labeled):\n",
        "    \"\"\"\n",
        "    Utility function, loads text file into a list of sentence and tag strings\n",
        "\n",
        "    Arguments:\n",
        "    - filename:     path to file\n",
        "    - is_labeled:   whether the file contains tags for each word or not\n",
        "        > if True, we assume each line is formatted as \"<word>\\t<tag>\\n\"\n",
        "        > if False, we assume each line is formatted as \"<word>\\n\"\n",
        "\n",
        "    Returns:\n",
        "    - sentences:    a list of sentences, where each sentence is a list \n",
        "                    words (strings)\n",
        "\n",
        "    if is_labeled=True, also returns\n",
        "    - tags:         a list of tags for each sentence, where tags[i] contains\n",
        "                    a list of tags (strings) that correspond to the words in \n",
        "                    sentences[i]\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    tags = []\n",
        "\n",
        "    current_sentence = []\n",
        "    current_tags = []\n",
        "\n",
        "    with open(filename, encoding='utf8') as f:\n",
        "      # iterate over the lines in the file\n",
        "      for line in f:\n",
        "        if len(line) == 0:\n",
        "          continue\n",
        "        if line == '\\n':\n",
        "          if len(current_sentence) != 0:\n",
        "            sentences.append(current_sentence)\n",
        "            tags.append(current_tags)\n",
        "\n",
        "          current_sentence = []\n",
        "          current_tags = []\n",
        "        else:\n",
        "          if is_labeled:\n",
        "            columns = line.rstrip().split('\\t')\n",
        "            word = columns[0].lower()\n",
        "            tag = columns[1]\n",
        "\n",
        "            current_sentence.append(word)\n",
        "            current_tags.append(tag)\n",
        "          else:\n",
        "            column = line.rstrip().split('\\t')\n",
        "            word = column[0].lower()\n",
        "            current_sentence.append(word)\n",
        "      \n",
        "      if is_labeled:\n",
        "        return sentences, tags\n",
        "      else:\n",
        "        return sentences\n",
        "\n",
        "  def get_batches(self, batch_size, vocab, tagset):\n",
        "    \"\"\"\n",
        "\n",
        "    Batches the data into mini-batches of size `batch_size`\n",
        "\n",
        "    Arguments:\n",
        "    - batch_size:       the desired output batch size\n",
        "    - vocab:            a dictionary mapping word strings to indices\n",
        "    - tagset:           a dictionary mapping tag strings to indices\n",
        "\n",
        "    Outputs:\n",
        "\n",
        "    if is_labeled=True:\n",
        "    - batched_word_indices:     a list of matrices of dimension (batch_size x max_seq_len)\n",
        "    - batched_tag_indices:      a list of matrices of dimension (batch_size x max_seq_len)\n",
        "    - batched_lengths:          a list of arrays of length (batch_size)\n",
        "\n",
        "    if is_labeled=False:\n",
        "    - batched_word_indices:     a list of matrices of dimension (batch_size x max_seq_len)\n",
        "    - batched_lengths:          a list of arrays of length (batch_size)\n",
        "\n",
        "\n",
        "    Description: \n",
        "\n",
        "    This function partitions the data into batches of size batch_size. If the number\n",
        "    of sentences in the document is not an even multiple of batch_size, the final batch\n",
        "    will contain the remaining elements. For example, if there are 82 sentences in the \n",
        "    dataset and batch_size=32, we return a list containing two batches of size 32 \n",
        "    and one final batch of size 18.\n",
        "\n",
        "    batched_word_indices[b] is a (batch_size x max_seq_len) matrix of integers, \n",
        "    containing index representations for sentences in the b-th batch in the document. \n",
        "    The `vocab` dictionary provides the correct mapping from word strings to indices. \n",
        "    If a word is not in the vocabulary, it gets mapped to UNKNOWN_INDEX (1).\n",
        "    `max_seq_len` is the maximum sentence length among the sentences in the current batch, \n",
        "    which will vary between different batches. All sentences shorter than max_seq_len \n",
        "    should be padded on the right with PAD_INDEX (0).\n",
        "\n",
        "    If the document is labeled, we also batch the document's tags. Analogous to \n",
        "    batched_word_indices, batched_tag_indices[b] contains the index representation\n",
        "    for the tags corresponding to the sentences in the b-th batch  in the document. \n",
        "    The `tagset` dictionary provides the correct mapping from tag strings to indicies. \n",
        "    All tag lists shorter than `max_seq_len` are padded with IGNORE_TAG_INDEX (-100).\n",
        "\n",
        "    batched_lengths[b] is a vector of length (batch_size). batched_lengths[b][i] \n",
        "    contains the original sentence length *before* padding for the i-th sentence\n",
        "    in the currrent batch. \n",
        "\n",
        "    \"\"\"\n",
        "    PAD_INDEX = 0             # reserved for padding words\n",
        "    UNKNOWN_INDEX = 1         # reserved for unknown words\n",
        "    IGNORE_TAG_INDEX = -100   # reserved for padding tags\n",
        "\n",
        "    # randomly shuffle the data\n",
        "    np.random.seed(159) # DON'T CHANGE THIS\n",
        "    shuffle = np.random.permutation(range(len(self.sentences)))\n",
        "\n",
        "    sentences = [self.sentences[i] for i in shuffle]\n",
        "    if self.is_labeled:\n",
        "      tags = [self.tags[i] for i in shuffle]\n",
        "    else:\n",
        "      tags = None\n",
        "\n",
        "    batched_word_indices = []\n",
        "    batched_tag_indices = []\n",
        "    batched_lengths = []\n",
        "\n",
        "    #############################\n",
        "    #       YOUR CODE HERE      #\n",
        "    #############################\n",
        "\n",
        "    \n",
        "\n",
        "    #############################\n",
        "    #       DO NOT MODIFY       #\n",
        "    #############################\n",
        "    if self.is_labeled:\n",
        "      return batched_word_indices, batched_tag_indices, batched_lengths\n",
        "    else:\n",
        "      return batched_word_indices, batched_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ePEcb46_zGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_tagset(tag_file):\n",
        "  \"\"\"\n",
        "  Utility function, loads tag file into a dictionary from tag string to tag index\n",
        "\n",
        "  Arguments:\n",
        "  - tag_file:   file location of the tagset\n",
        "\n",
        "  Outputs:\n",
        "  - tagset:     a dictionary mapping tag strings (e.g. \"VB\") to a unique index\n",
        "  \"\"\"\n",
        "  tagset = {}\n",
        "  with open(tag_file, encoding='utf8') as f:\n",
        "    for line in f:\n",
        "      columns = line.rstrip().split('\\t')\n",
        "      tag = columns[0]\n",
        "      tag_id = int(columns[1])\n",
        "      tagset[tag] = tag_id\n",
        "  \n",
        "  return tagset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mMQIJ_vVuiz",
        "colab_type": "text"
      },
      "source": [
        "The cells below download the data files and construct the corresponding `Dataset` objects. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhXTkpTqGR1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp20/master/HW_4/pos.train\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp20/master/HW_4/pos.dev\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp20/master/HW_4/pos.test\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp20/master/HW_4/pos.tagset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vtP5seIgBORT",
        "colab": {}
      },
      "source": [
        "# read the files\n",
        "tagset = read_tagset('pos.tagset')\n",
        "train_dataset = Dataset('pos.train', is_labeled=True)\n",
        "dev_dataset = Dataset('pos.dev', is_labeled=True)\n",
        "test_dataset = Dataset('pos.test', is_labeled=False)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# these should run without errors if implemented correctly\n",
        "train_batch_idx, train_batch_tags, train_batch_lens = train_dataset.get_batches(BATCH_SIZE, vocab, tagset)\n",
        "dev_batch_idx, dev_batch_tags, dev_batch_lens = dev_dataset.get_batches(BATCH_SIZE, vocab, tagset)\n",
        "test_batch_idx, test_batch_lens = test_dataset.get_batches(BATCH_SIZE, vocab, tagset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-nnX7WxqDJ3",
        "colab_type": "text"
      },
      "source": [
        "### Part 2: Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMtkCBt_wzIS",
        "colab_type": "text"
      },
      "source": [
        "Next, we will implement utility functions that will later be used to assess our model's perfomance. \n",
        "\n",
        "**Please make sure that**\n",
        "\n",
        "*   Your implementation is self-contained. That is, keep all helper functions or variables inside of your function.\n",
        "*   Your implementation does not import any additional libraries. You will not receive credit if you do."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQLiM0ukG-4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The accuracy function has been implemented for you\n",
        "\n",
        "def accuracy(true, pred):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  - true:       a list of true label values (integers)\n",
        "  - pred:       a list of predicted label values (integers)\n",
        "\n",
        "  Output:\n",
        "  - accuracy:   the prediction accuracy\n",
        "  \"\"\"\n",
        "  true = np.array(true)\n",
        "  pred = np.array(pred)\n",
        "\n",
        "  num_correct = sum(true == pred)\n",
        "  num_total = len(true)\n",
        "  return num_correct / num_total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChJjUu45qFM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def confusion_matrix(true, pred, num_tags):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  - true:       a list of true label values (integers)\n",
        "  - pred:       a list of predicted label values (integers)\n",
        "  - num_tags:   the number of possible tags\n",
        "                true and pred will both contain integers between\n",
        "                0 and num_tags - 1 (inclusive)\n",
        "\n",
        "  Output: \n",
        "  - confusion_matrix:   a (num_tags x num_tags) matrix of integers\n",
        "\n",
        "  confusion_matrix[i][j] = # predictions where true label\n",
        "  was i and predicted label was j\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  confusion_matrix = np.zeros((num_tags, num_tags))\n",
        "\n",
        "  #############################\n",
        "  #       YOUR CODE HERE      #\n",
        "  #############################\n",
        "\n",
        "  \n",
        "\n",
        "  return confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Hdj6QSaBV9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def precision(true, pred, num_tags):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  - true:       a list of true label values (integers)\n",
        "  - pred:       a list of predicted label values (integers)\n",
        "  - num_tags:   the number of possible tags\n",
        "                true and pred will both contain integers between\n",
        "                0 and num_tags - 1 (inclusive)\n",
        "\n",
        "  Output: \n",
        "  - precision:  an array of length num_tags, where precision[i]\n",
        "                gives the precision of class i\n",
        "\n",
        "  Hints:  the confusion matrix may be useful\n",
        "          be careful about zero division\n",
        "  \"\"\"\n",
        "\n",
        "  precision = np.zeros(num_tags)\n",
        "\n",
        "  #############################\n",
        "  #       YOUR CODE HERE      #\n",
        "  #############################\n",
        "\n",
        "\n",
        "  \n",
        "  return precision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJL55TnOBVxl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def recall(true, pred, num_tags):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  - true:       a list of true label values (integers)\n",
        "  - pred:       a list of predicted label values (integers)\n",
        "  - num_tags:   the number of possible tags\n",
        "                true and pred will both contain integers between\n",
        "                0 and num_tags - 1 (inclusive)\n",
        "\n",
        "  Output: \n",
        "  - recall:     an array of length num_tags, where recall[i]\n",
        "                gives the recall of class i\n",
        "\n",
        "  Hints:  the confusion matrix may be useful\n",
        "          be careful about zero division\n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\"\n",
        "  YOUR CODE HERE\n",
        "  \"\"\"\n",
        "  recall = np.zeros(num_tags)\n",
        "\n",
        "  #############################\n",
        "  #       YOUR CODE HERE      #\n",
        "  #############################\n",
        "\n",
        "\n",
        "  return recall"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7drr7z1VBVjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f1_score(true, pred, num_tags):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  - true:       a list of true label values (integers)\n",
        "  - pred:       a list of predicted label values (integers)\n",
        "  - num_tags:   the number of possible tags\n",
        "                true and pred will both contain integers between\n",
        "                0 and num_tags - 1 (inclusive)\n",
        "\n",
        "  Output: \n",
        "  - f1:         an array of length num_tags, where f1[i]\n",
        "                gives the recall of class i\n",
        "  \"\"\"\n",
        "  f1 = np.zeros(num_tags)\n",
        "\n",
        "  #############################\n",
        "  #       YOUR CODE HERE      #\n",
        "  #############################\n",
        "\n",
        "\n",
        "  return f1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS1p55P5UGv4",
        "colab_type": "text"
      },
      "source": [
        "### Part 3: Building the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IPoFwqfoFOO",
        "colab_type": "text"
      },
      "source": [
        "Fill in the blanks in `LSTMTagger`'s `__init__` function. If you get stuck, you can reference PyTorch's [torch.nn documentation](https://pytorch.org/docs/stable/nn.html) or [this official tutorial](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html) on LSTM sequence labeling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6J3z3T0USI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "  \"\"\"\n",
        "  An LSTM model for sequence labeling\n",
        "\n",
        "  Initialization Arguments:\n",
        "  - embeddings:   a matrix of size (vocab_size, emb_dim)\n",
        "                  containing pretrained embedding weights\n",
        "  - hidden_dim:   the LSTM's hidden layer size\n",
        "  - tagset_size:  the number of possible output tags\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, embeddings, hidden_dim, tagset_size):\n",
        "    super().__init__()\n",
        "  \n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.num_labels = tagset_size\n",
        "\n",
        "    #############################\n",
        "    #       YOUR CODE HERE      #\n",
        "    #############################\n",
        "\n",
        "    # Initialize a PyTorch embeddings layer using the pretrained embedding weights\n",
        "    self.embeddings = ... \n",
        "\n",
        "    # Initialize an LSTM layer\n",
        "    self.lstm = ...\n",
        "\n",
        "    # Initialize a single feedforward layer\n",
        "    self.hidden2tag = ...\n",
        "  \n",
        "  def forward(self, indices, lengths):\n",
        "    \"\"\"\n",
        "    Runs a batched sequence through the model and returns output logits\n",
        "\n",
        "    Arguments:\n",
        "    - indices:  a matrix of size (batch_size x max_seq_len)\n",
        "                containing the word indices of sentences in the batch\n",
        "    - lengths:  a vector of size (batch_size) containing the\n",
        "                original lengths of the sequences before padding\n",
        "\n",
        "    Output:\n",
        "    - logits:   a matrix of size (batch_size x max_seq_len x num_tags)\n",
        "                gives a score to each possible tag for each word\n",
        "                in each sentence \n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # cast arrays as PyTorch data types and move to GPU memory\n",
        "    indices = torch.LongTensor(indices).to(device)\n",
        "    lengths = torch.LongTensor(lengths).to(device)\n",
        "    \n",
        "    # convert word indices to word embeddings\n",
        "    embeddings = self.embeddings(indices)\n",
        "\n",
        "    # pack/pad handles variable length sequence batching\n",
        "    # see here if you're curious: https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec\n",
        "    packed_input_embs = pack_padded_sequence(embeddings, lengths, batch_first=True, enforce_sorted=False)\n",
        "    # run input through LSTM layer\n",
        "    packed_output, _ = self.lstm(packed_input_embs)\n",
        "    # unpack sequences into original format\n",
        "    padded_output, output_lengths = pad_packed_sequence(packed_output, batch_first=True)\n",
        "\n",
        "    logits = self.hidden2tag(padded_output)\n",
        "    return logits\n",
        "\n",
        "  def run_training(self, train_dataset, dev_dataset, batch_size, vocab, tagset,\n",
        "                         lr=5e-4, num_epochs=100, eval_every=5):\n",
        "    \"\"\"\n",
        "    Trains the model on the training data with a learning rate of lr\n",
        "    for num_epochs. Evaluates the model on the dev data eval_every epochs.\n",
        "\n",
        "    Arguments:\n",
        "    - train_dataset:  Dataset object containing the training data\n",
        "    - dev_dataset:    Dataset object containing the dev data\n",
        "    - batch_size:     batch size for train/dev data\n",
        "    - vocab:          a dictionary mapping word strings to indices\n",
        "    - tagset:         a dictionary mapping tag strings to indices\n",
        "    - lr:             learning rate\n",
        "    - num_epochs:     number of epochs to train for\n",
        "    - eval_every:     evaluation is run eval_every epochs\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    if str(device) == 'cpu':\n",
        "      print(\"Training only supported in GPU environment\")\n",
        "      return\n",
        "\n",
        "    # clear unreferenced data/models from GPU memory \n",
        "    torch.cuda.empty_cache()\n",
        "    # move model to GPU memory\n",
        "    self.to(device)\n",
        "\n",
        "    # set the optimizer (Adam) and loss function (CrossEnt)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_function = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    # batch training and dev data\n",
        "    train_batch_idx, train_batch_tags, train_batch_lens = train_dataset.get_batches(BATCH_SIZE, vocab, tagset)\n",
        "    dev_batch_idx, dev_batch_tags, dev_batch_lens = dev_dataset.get_batches(BATCH_SIZE, vocab, tagset)\n",
        "\n",
        "    print(\"**** TRAINING *****\")\n",
        "    for i in range(num_epochs):\n",
        "      # sets the model in train mode\n",
        "      self.train()\n",
        "\n",
        "      total_loss = 0\n",
        "      for b in range(len(train_batch_idx)):\n",
        "        # compute the logits\n",
        "        logits = model.forward(train_batch_idx[b], train_batch_lens[b])\n",
        "        # move labels to GPU memory\n",
        "        labels = torch.LongTensor(train_batch_tags[b]).to(device)\n",
        "        # compute the loss with respect to true labels\n",
        "        loss = loss_function(logits.view(-1, len(tagset)), labels.view(-1))\n",
        "        total_loss += loss\n",
        "        # propagate gradients backward\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # set model gradients to zero before performing next forward pass\n",
        "        self.zero_grad()\n",
        "\n",
        "      print(\"Epoch {} | Loss: {}\".format(i, total_loss))\n",
        "\n",
        "      if (i + 1) % eval_every == 0:\n",
        "        print(\"**** EVALUATION *****\")\n",
        "        # sets the model in evaluate mode (no gradients)\n",
        "        self.eval()\n",
        "        # compute dev f1 score\n",
        "        acc, true, pred = self.evaluate(dev_batch_idx, dev_batch_lens, dev_batch_tags, tagset)\n",
        "        print(\"Dev Accuracy: {}\".format(acc))\n",
        "        print(\"**********************\")\n",
        "\n",
        "  def evaluate(self, batched_sentences, batched_lengths, batched_labels, tagset):\n",
        "    \"\"\"\n",
        "    Evaluate the model's predictions on the provided dataset. \n",
        "\n",
        "    Arguments:\n",
        "    - batched_sentences:  a list of matrices, each of size (batch_size x max_seq_len),\n",
        "                          containing the word indices of sentences in the batch\n",
        "    - batched_lengths:    a list of vectors, each of size (batch_size), containing the\n",
        "                          original lengths of the sequences before padding\n",
        "    - batched_labels:     a list of matrices, each of size (batch_size x max_seq_len),\n",
        "                          containing the tag indices corresponding to sentences in the batch\n",
        "    - num_tags:           the number of possible output tags\n",
        "\n",
        "    Output:\n",
        "    - accuracy:           the model's prediction accuracy\n",
        "    - all_true_labels:    a flattened list of all true labels\n",
        "    - all_predictions:    a flattened list of all of the model's corresponding predictions\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    all_true_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    for b in range(len(batched_sentences)):\n",
        "      logits = self.forward(batched_sentences[b], batched_lengths[b])\n",
        "      batch_predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "\n",
        "      batch_size, _ = batched_sentences[b].shape\n",
        "\n",
        "      for i in range(batch_size):\n",
        "        tags = batched_labels[b][i]\n",
        "        preds = batch_predictions[i]\n",
        "        \n",
        "        seq_len = int(batched_lengths[b][i])\n",
        "        for j in range(seq_len):\n",
        "          all_predictions.append(int(preds[j]))\n",
        "          all_true_labels.append(int(tags[j]))\n",
        "      \n",
        "    \n",
        "    acc = accuracy(all_true_labels, all_predictions)\n",
        "      \n",
        "    return acc, all_true_labels, all_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4AOB94R9RFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_seed(seed):\n",
        "  \"\"\"\n",
        "  Sets random seeds and sets model in deterministic\n",
        "  training mode. Ensures reproducible results\n",
        "  \"\"\"\n",
        "  torch.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  np.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AuNeDk9qAM_",
        "colab_type": "text"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WTEvLeVuNWl",
        "colab_type": "text"
      },
      "source": [
        "Run the cells below to train your model. If all of the previous sections are implemented correctly, you should see\n",
        "\n",
        "\n",
        "*   the loss decreasing consistently for every epoch\n",
        "*   the dev accuracy increasing until convergence around ~0.88\n",
        "\n",
        "The staff solution achieves an accuracy of 0.880 after 25 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9NqwYnfU2WB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sets the random seed – DO NOT change this\n",
        "# this ensures deterministic results that are comparable with the staff values\n",
        "set_seed(159)\n",
        "\n",
        "HIDDEN_SIZE = 64\n",
        "# intialize a new LSTMTagger model\n",
        "model = LSTMTagger(embeddings, HIDDEN_SIZE, len(tagset))\n",
        "# train the model\n",
        "model.run_training(train_dataset, dev_dataset, BATCH_SIZE, vocab, tagset,   \n",
        "                   lr=5e-4, num_epochs=25, eval_every=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH5giKgvJp0c",
        "colab_type": "text"
      },
      "source": [
        "Once the model is trained, run the cells below to print the precision, recall, and $F_1$ score per class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLqaZ_6cMMPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_per_class(model, dataset, vocab, tagset):\n",
        "  \"\"\"\n",
        "  Prints precision, recall, and F1 for each class in the tagset\n",
        "  \"\"\"\n",
        "  # batch the data\n",
        "  batched_idx, batched_tags, batched_lens = dev_dataset.get_batches(BATCH_SIZE, vocab, tagset)\n",
        "  # compute idx --> tag from tag --> idx\n",
        "  reverse_tagset = {v: k for k,v in tagset.items()}\n",
        "  # evaluate model on hold-out set\n",
        "  acc, true, pred = model.evaluate(batched_idx, batched_lens, batched_tags, tagset)\n",
        "  true = np.array(true)\n",
        "  pred = np.array(pred)\n",
        "\n",
        "  pr = precision(true, pred, len(tagset))\n",
        "  re = recall(true, pred, len(tagset))\n",
        "  f1 = f1_score(true, pred, len(tagset))\n",
        "\n",
        "  for idx, tag in reverse_tagset.items():\n",
        "    print(\"***********************\")\n",
        "    print(\"TAG: {}\".format(tag))\n",
        "    num_pred = np.sum(pred == idx)\n",
        "    num_true = np.sum(true == idx)\n",
        "    print(\"({} pred, {} true)\".format(num_pred, num_true))\n",
        "\n",
        "    print(\"PRECISION: \\t{:.3f}\".format(pr[idx]))\n",
        "    print(\"RECALL: \\t{:.3f}\".format(re[idx]))\n",
        "    print(\"F1 SCORE: \\t{:.3f}\".format(f1[idx]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tTEpCBsuYuP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_per_class(model, dev_dataset, vocab, tagset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lefzFwCD8AJU",
        "colab_type": "text"
      },
      "source": [
        "## Part 4: Model Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxZ_Wn75uw7n",
        "colab_type": "text"
      },
      "source": [
        "Congratulations, you've just trained a neural network!\n",
        "\n",
        "Now, improve the `LSTMTagger` model and implementing the `init` function in the `FancyTagger` class below. \n",
        "* Feel free to replace the `forward` function inherited from `LSTMTagger` if \n",
        "you need to, but it should not be necessary to receive full credit. Credit will be awarded based on the performance on a holdout test set. \n",
        "* Do not modify any of the cells above when completing part 4. Instead, insert cells below if you need to perform any additional computations. \n",
        "* You are allowed to use any function in `torch.nn`. You are **not** allowed to import any libraries or use implementations copied from the internet. \n",
        "\n",
        "Before submitting, please describe your modifications below:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ya-aaGh6l8D",
        "colab_type": "text"
      },
      "source": [
        "[ YOUR TEXT HERE ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKz2PLbu5d8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FancyTagger(LSTMTagger):\n",
        "  \"\"\"\n",
        "  An improved neural model for sequence labeling\n",
        "\n",
        "  Starter code from LSTMTagger has already been provided, but\n",
        "  feel free to change the init and forward function internals\n",
        "  if your model design requires it (though this is not necessary\n",
        "  to receive full credit).\n",
        "\n",
        "  You may use any component in torch.nn. You may NOT\n",
        "  import any additional libraries/modules. \n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, embeddings, hidden_dim, tagset_size):\n",
        "    # initializes the parent LSTMTagger class\n",
        "    # inherits forward, evaluate, and run_training methods\n",
        "    super().__init__(embeddings, hidden_dim, tagset_size)\n",
        "  \n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.num_labels = tagset_size\n",
        "\n",
        "    #############################\n",
        "    #       YOUR CODE HERE      #\n",
        "    #############################\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGDY4ymJvo3h",
        "colab_type": "text"
      },
      "source": [
        "Run the training script below to train the `FancyTagger` model. Again, feel free to adjust any hyperparameters if necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lnp-tWl9Vbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = FancyTagger(embeddings, HIDDEN_SIZE, len(tagset))\n",
        "print(model)\n",
        "model.run_training(train_dataset, dev_dataset, BATCH_SIZE, vocab, tagset,   \n",
        "                   lr=5e-4, num_epochs=100, eval_every=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgLM__WZw4wz",
        "colab_type": "text"
      },
      "source": [
        "### Save Predictions\n",
        "\n",
        "When you are satisfied with your `FancyTagger`'s performance on the dev set, run the cell below to write your predictions on the test set to a text file. \n",
        "\n",
        "You can download `predictions.txt` by going to \n",
        "**View > Table of Contents > Files**\n",
        "\n",
        "Please submit this `predictions.txt` file to Gradescope. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddSD3-FN9Zzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert isinstance(model, FancyTagger), 'Please assign your FancyTagger to a variable named model'\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "test_batch_idx, test_batch_lens = test_dataset.get_batches(BATCH_SIZE, vocab, tagset)\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for b in range(len(test_batch_idx)):\n",
        "  logits = model.forward(test_batch_idx[b], test_batch_lens[b])\n",
        "  batch_predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "\n",
        "  batch_size, _ = test_batch_idx[b].shape\n",
        "\n",
        "  for i in range(batch_size):\n",
        "    preds = batch_predictions[i]\n",
        "    \n",
        "    seq_len = int(test_batch_lens[b][i])\n",
        "    for j in range(seq_len):\n",
        "      predictions.append(int(preds[j]))\n",
        "  \n",
        "\n",
        "with open('predictions.txt', 'w') as f:\n",
        "  for p in predictions:\n",
        "    f.write(str(p) + \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}